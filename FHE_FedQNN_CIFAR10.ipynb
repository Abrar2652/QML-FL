{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e21f4aa6",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tenseal syft pennylane\n",
    "!pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62316493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "import syft as sy\n",
    "import pickle\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Dict, Optional, Callable, Union, cast\n",
    "import tenseal as ts\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import syft as sy\n",
    "from logging import WARNING\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b606a18",
   "metadata": {},
   "source": [
    "# Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb31c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choice_device(device):\n",
    "    if torch.cuda.is_available() and device != \"cpu\":\n",
    "        device = \"cuda:0\"\n",
    "    elif (\n",
    "        torch.backends.mps.is_available()\n",
    "        and torch.backends.mps.is_built()\n",
    "        and device != \"cpu\"\n",
    "    ):\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    return device\n",
    "\n",
    "\n",
    "def classes_string(name_dataset):\n",
    "    if name_dataset == \"cifar\":\n",
    "        return (\n",
    "            \"plane\",\n",
    "            \"car\",\n",
    "            \"bird\",\n",
    "            \"cat\",\n",
    "            \"deer\",\n",
    "            \"dog\",\n",
    "            \"frog\",\n",
    "            \"horse\",\n",
    "            \"ship\",\n",
    "            \"truck\",\n",
    "        )\n",
    "    elif name_dataset == \"MRI\":\n",
    "        return (\"glioma\", \"meningioma\", \"notumor\", \"pituitary\")\n",
    "    else:\n",
    "        print(\"Warning: unspecified dataset\")\n",
    "        return ()\n",
    "\n",
    "\n",
    "def save_matrix(y_true, y_pred, path, classes):\n",
    "    y_true_mapped = [classes[label] for label in y_true]\n",
    "    y_pred_mapped = [classes[label] for label in y_pred]\n",
    "    cf_matrix_normalized = confusion_matrix(\n",
    "        y_true_mapped, y_pred_mapped, labels=classes, normalize=\"all\"\n",
    "    )\n",
    "    cf_matrix_round = np.round(cf_matrix_normalized, 2)\n",
    "    df_cm = pd.DataFrame(\n",
    "        cf_matrix_round, index=[i for i in classes], columns=[i for i in classes]\n",
    "    )\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.xlabel(\"Predicted label\", fontsize=13)\n",
    "    plt.ylabel(\"True label\", fontsize=13)\n",
    "    plt.title(\"Confusion Matrix\", fontsize=15)\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_roc(targets, y_proba, path, nbr_classes):\n",
    "    y_true = np.zeros(shape=(len(targets), nbr_classes))\n",
    "    for i in range(len(targets)):\n",
    "        y_true[i, targets[i]] = 1\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(nbr_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(nbr_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(nbr_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= nbr_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    plt.figure()\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=f\"micro-average ROC curve (area = {roc_auc['micro']:.2f})\",\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "    plt.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=f\"macro-average ROC curve (area = {roc_auc['macro']:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "    lw = 2\n",
    "    for i in range(nbr_classes):\n",
    "        plt.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            lw=lw,\n",
    "            label=f\"ROC curve of class {i} (area = {roc_auc[i]:.2f})\",\n",
    "        )\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=lw, label=\"Worst case\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic (ROC) Curve OvR\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_graphs(path_save, local_epoch, results, end_file=\"\"):\n",
    "    os.makedirs(path_save, exist_ok=True)\n",
    "    print(\"Saving graphs in \", path_save)\n",
    "    plot_graph(\n",
    "        [[*range(local_epoch)]] * 2,\n",
    "        [results[\"train_acc\"], results[\"val_acc\"]],\n",
    "        \"Epochs\",\n",
    "        \"Accuracy (%)\",\n",
    "        [\"Training accuracy\", \"Validation accuracy\"],\n",
    "        \"Accuracy curves\",\n",
    "        path_save + \"Accuracy_curves\" + end_file,\n",
    "    )\n",
    "    plot_graph(\n",
    "        [[*range(local_epoch)]] * 2,\n",
    "        [results[\"train_loss\"], results[\"val_loss\"]],\n",
    "        \"Epochs\",\n",
    "        \"Loss\",\n",
    "        [\"Training loss\", \"Validation loss\"],\n",
    "        \"Loss curves\",\n",
    "        path_save + \"Loss_curves\" + end_file,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_graph(\n",
    "    list_xplot, list_yplot, x_label, y_label, curve_labels, title, path=None\n",
    "):\n",
    "    lw = 2\n",
    "    plt.figure()\n",
    "    for i in range(len(curve_labels)):\n",
    "        plt.plot(list_xplot[i], list_yplot[i], lw=lw, label=curve_labels[i])\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    if curve_labels:\n",
    "        plt.legend(loc=\"lower right\")\n",
    "    if path:\n",
    "        plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def get_parameters2(net, context_client=None) -> List[np.ndarray]:\n",
    "    if context_client:\n",
    "        encrypted_tensor = crypte(net.state_dict(), context_client)\n",
    "        return [layer.get_weight() for layer in encrypted_tensor]\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray], context_client=None):\n",
    "    state_dict = net.state_dict()  # Get the model's state dictionary for shapes\n",
    "    params_dict = zip(state_dict.keys(), parameters)\n",
    "    if context_client:\n",
    "        secret_key = context_client.secret_key()\n",
    "        dico = {k: deserialized_layer(k, v, context_client) for k, v in params_dict}\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in dico.items():\n",
    "            if isinstance(v, CryptedLayer):\n",
    "                decrypted = v.decrypt(secret_key)  # Returns a flattened list\n",
    "                shape = state_dict[k].shape  # Get the expected shape\n",
    "                # Reshape the decrypted list to match the original tensor shape\n",
    "                new_state_dict[k] = torch.Tensor(np.array(decrypted).reshape(shape))\n",
    "            else:\n",
    "                new_state_dict[k] = torch.Tensor(v.get_weight())  # Plain parameters\n",
    "    else:\n",
    "        new_state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(new_state_dict, strict=True)\n",
    "    print(\"Updated model parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e447a",
   "metadata": {},
   "source": [
    "# Security-related classes and functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, name_layer, weight):\n",
    "        self.name = name_layer\n",
    "        self.weight_array = weight\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def get_weight(self):\n",
    "        return self.weight_array\n",
    "\n",
    "    def __add__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        return Layer(self.name, self.weight_array + weights)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        return Layer(self.name, self.weight_array - weights)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        return Layer(self.name, self.weight_array * weights)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        weights = self.weight_array * (1 / weights)\n",
    "        return Layer(self.name, weights)\n",
    "\n",
    "    def __len__(self):\n",
    "        somme = 1\n",
    "        for elem in self.weight_array.shape:\n",
    "            somme *= elem\n",
    "        return somme\n",
    "\n",
    "    def shape(self):\n",
    "        return self.weight_array.shape\n",
    "\n",
    "    def sum(self, axis=0):\n",
    "        return Layer(f\"sum_{self.name}\", self.weight_array.sum(axis=axis))\n",
    "\n",
    "    def mean(self, axis=0):\n",
    "        weights = self.weight_array.sum(axis=axis) * (1 / self.weight_array.shape[axis])\n",
    "        return Layer(f\"sum_{self.name}\", weights)\n",
    "\n",
    "    def decrypt(self, sk=None):\n",
    "        return self.weight_array.tolist()\n",
    "\n",
    "    def serialize(self):\n",
    "        return {self.name: self.weight_array}\n",
    "\n",
    "\n",
    "class CryptedLayer(Layer):\n",
    "    def __init__(self, name_layer, weight, contexte=None):\n",
    "        super(CryptedLayer, self).__init__(name_layer, weight)\n",
    "        if isinstance(weight, (ts.tensors.CKKSTensor, bytes)):\n",
    "            self.weight_array = weight\n",
    "        else:\n",
    "            self.weight_array = ts.ckks_tensor(contexte, weight.cpu().detach().numpy())\n",
    "\n",
    "    def __add__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "        return CryptedLayer(self.name, self.weight_array + weights)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "        return CryptedLayer(self.name, self.weight_array - weights)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "        return CryptedLayer(self.name, self.weight_array * weights)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        try:\n",
    "            weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "            weights = self.weight_array * (1 / weights)\n",
    "        except:\n",
    "            print(\"Error: division operator not supported by SEAL\")\n",
    "            weights = []\n",
    "        return CryptedLayer(self.name, weights)\n",
    "\n",
    "    def shape(self):\n",
    "        return self.weight_array.shape\n",
    "\n",
    "    def sum(self, axis=0):\n",
    "        return CryptedLayer(f\"sum_{self.name}\", self.weight_array.sum(axis=axis))\n",
    "\n",
    "    def mean(self, axis=0):\n",
    "        weights = self.weight_array.sum(axis=axis) * (1 / self.weight_array.shape[axis])\n",
    "        return CryptedLayer(f\"sum_{self.name}\", weights)\n",
    "\n",
    "    def decrypt(self, sk=None):\n",
    "        return (\n",
    "            self.weight_array.decrypt(sk).tolist()\n",
    "            if sk\n",
    "            else self.weight_array.decrypt().tolist()\n",
    "        )\n",
    "\n",
    "    def serialize(self):\n",
    "        return {self.name: self.weight_array.serialize()}\n",
    "\n",
    "\n",
    "def context():\n",
    "    cont = ts.context(\n",
    "        ts.SCHEME_TYPE.CKKS,\n",
    "        poly_modulus_degree=8192,\n",
    "        coeff_mod_bit_sizes=[60, 40, 40, 60],\n",
    "    )\n",
    "    cont.generate_galois_keys()\n",
    "    cont.global_scale = 2**40\n",
    "    return cont\n",
    "\n",
    "\n",
    "def crypte(client_w, context_c):\n",
    "    encrypted = []\n",
    "    for name_layer, weight_array in client_w.items():\n",
    "        if name_layer == \"qnn.weights\":  # Encrypt quantum layer parameters\n",
    "            encrypted.append(CryptedLayer(name_layer, weight_array, context_c))\n",
    "        else:\n",
    "            encrypted.append(Layer(name_layer, weight_array))\n",
    "    return encrypted\n",
    "\n",
    "\n",
    "def read_query(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            query_str = pickle.load(file)\n",
    "        contexte = query_str[\"contexte\"]\n",
    "        del query_str[\"contexte\"]\n",
    "        return query_str, contexte\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def write_query(file_path, client_query):\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        encode_str = pickle.dumps(client_query)\n",
    "        file.write(encode_str)\n",
    "\n",
    "\n",
    "def deserialized_layer(name_layer, weight_array, ctx):\n",
    "    if isinstance(weight_array, bytes):\n",
    "        return CryptedLayer(name_layer, ts.ckks_tensor_from(ctx, weight_array), ctx)\n",
    "    elif isinstance(weight_array, ts.tensors.CKKSTensor):\n",
    "        return CryptedLayer(name_layer, weight_array, ctx)\n",
    "    else:\n",
    "        return Layer(name_layer, weight_array)\n",
    "\n",
    "\n",
    "def serialize_ndarray(ndarray):\n",
    "    if isinstance(ndarray, ts.tensors.CKKSTensor):\n",
    "        return ndarray.serialize()\n",
    "    elif isinstance(ndarray, torch.Tensor):\n",
    "        # Move tensor to CPU and convert to NumPy array\n",
    "        return serialize_ndarray(ndarray.cpu().detach().numpy())\n",
    "    else:\n",
    "        bytes_io = BytesIO()\n",
    "        np.save(bytes_io, ndarray, allow_pickle=False)\n",
    "        return bytes_io.getvalue()\n",
    "\n",
    "\n",
    "def deserialize_ndarray(tensor, context):\n",
    "    try:\n",
    "        return ts.ckks_tensor_from(context, tensor)\n",
    "    except:\n",
    "        bytes_io = BytesIO(tensor)\n",
    "        return np.load(bytes_io, allow_pickle=False)\n",
    "\n",
    "\n",
    "def serialize_parameters(parameters):\n",
    "    return [serialize_ndarray(param) for param in parameters]\n",
    "\n",
    "\n",
    "def deserialize_parameters(serialized_params, context):\n",
    "    return [deserialize_ndarray(param, context) for param in serialized_params]\n",
    "\n",
    "\n",
    "def aggregate_serialized(results, context):\n",
    "    num_examples_total = sum([num_examples for _, num_examples in results])\n",
    "    weights_results = [\n",
    "        (deserialize_parameters(serialized_params, context), num_examples)\n",
    "        for serialized_params, num_examples in results\n",
    "    ]\n",
    "    aggregated_params = []\n",
    "    for layer_idx in range(len(weights_results[0][0])):\n",
    "        layer_updates = [weights[layer_idx] for weights, _ in weights_results]\n",
    "        if isinstance(layer_updates[0], ts.tensors.CKKSTensor):\n",
    "            weighted_sum = sum(\n",
    "                [\n",
    "                    layer * num_examples\n",
    "                    for layer, num_examples in zip(\n",
    "                        layer_updates, [num for _, num in weights_results]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            aggregated_layer = weighted_sum * (1 / num_examples_total)\n",
    "        else:\n",
    "            weighted_sum = sum(\n",
    "                [\n",
    "                    layer * num_examples\n",
    "                    for layer, num_examples in zip(\n",
    "                        layer_updates, [num for _, num in weights_results]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            aggregated_layer = weighted_sum / num_examples_total\n",
    "        aggregated_params.append(aggregated_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32a729",
   "metadata": {},
   "source": [
    "# Data setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fcbb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_DICT = {\n",
    "    \"cifar\": dict(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    \"MRI\": dict(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "}\n",
    "\n",
    "\n",
    "def split_data_client(dataset, num_clients, seed):\n",
    "    partition_size = len(dataset) // num_clients\n",
    "    lengths = [partition_size] * (num_clients - 1)\n",
    "    lengths += [len(dataset) - sum(lengths)]\n",
    "    ds = random_split(dataset, lengths, torch.Generator().manual_seed(seed))\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    num_clients: int,\n",
    "    batch_size: int,\n",
    "    resize: int,\n",
    "    seed: int,\n",
    "    num_workers: int,\n",
    "    splitter=10,\n",
    "    dataset=\"cifar\",\n",
    "    data_path=\"./data/\",\n",
    "    data_path_val=\"\",\n",
    "):\n",
    "    list_transforms = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**NORMALIZE_DICT[dataset]),\n",
    "    ]\n",
    "    if dataset != \"cifar\" and resize is not None:\n",
    "        list_transforms = [transforms.Resize((resize, resize))] + list_transforms\n",
    "    transformer = transforms.Compose(list_transforms)\n",
    "    if dataset == \"cifar\":\n",
    "        trainset = datasets.CIFAR10(\n",
    "            data_path + dataset, train=True, download=True, transform=transformer\n",
    "        )\n",
    "        testset = datasets.CIFAR10(\n",
    "            data_path + dataset, train=False, download=True, transform=transformer\n",
    "        )\n",
    "    else:\n",
    "        trainset = datasets.ImageFolder(\n",
    "            data_path + dataset + \"/Training\", transform=transformer\n",
    "        )\n",
    "        testset = datasets.ImageFolder(\n",
    "            data_path + dataset + \"/Testing\", transform=transformer\n",
    "        )\n",
    "    datasets_train = split_data_client(trainset, num_clients, seed)\n",
    "    if data_path_val:\n",
    "        valset = datasets.ImageFolder(data_path_val, transform=transformer)\n",
    "        datasets_val = split_data_client(valset, num_clients, seed)\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for i in range(num_clients):\n",
    "        if data_path_val:\n",
    "            trainloaders.append(\n",
    "                DataLoader(datasets_train[i], batch_size=batch_size, shuffle=True)\n",
    "            )\n",
    "            valloaders.append(DataLoader(datasets_val[i], batch_size=batch_size))\n",
    "        else:\n",
    "            len_val = int(len(datasets_train[i]) * splitter / 100)\n",
    "            len_train = len(datasets_train[i]) - len_val\n",
    "            lengths = [len_train, len_val]\n",
    "            ds_train, ds_val = random_split(\n",
    "                datasets_train[i], lengths, torch.Generator().manual_seed(seed)\n",
    "            )\n",
    "            trainloaders.append(\n",
    "                DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "            )\n",
    "            valloaders.append(DataLoader(ds_val, batch_size=batch_size))\n",
    "    testloader = DataLoader(testset, batch_size=batch_size)\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124621e2",
   "metadata": {},
   "source": [
    "# Training and testing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f71631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: Union[torch.nn.Module, Tuple],\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_proba = []\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images)\n",
    "            probas_output = softmax(output)\n",
    "            y_proba.extend(probas_output.detach().cpu().numpy())\n",
    "            loss = loss_fn(output, labels)\n",
    "            test_loss += loss.item()\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            y_pred.extend(preds)\n",
    "            acc = (preds == labels).mean()\n",
    "            test_acc += acc\n",
    "    y_proba = np.array(y_proba)\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc * 100, y_pred, y_true, y_proba\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: Union[torch.nn.Module, Tuple],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred_class = torch.argmax(torch.softmax(output, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == labels).sum().item() / len(output)\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc * 100\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Union[torch.nn.Module, Tuple],\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, List]:\n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(\n",
    "            model, train_dataloader, loss_fn, optimizer, device\n",
    "        )\n",
    "        val_loss, val_acc, *_ = test(model, test_dataloader, loss_fn, device)\n",
    "        print(\n",
    "            f\"\\tTrain Epoch: {epoch + 1} \\tTrain_loss: {train_loss:.4f} | Train_acc: {train_acc:.4f} % | \"\n",
    "            f\"Validation_loss: {val_loss:.4f} | Validation_acc: {val_acc:.4f} %\"\n",
    "        )\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77804e",
   "metadata": {},
   "source": [
    "# Main experiment setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "he = True\n",
    "data_path = \"data/\"\n",
    "dataset = \"cifar\"\n",
    "yaml_path = \"./results/FL/results.yml\"\n",
    "seed = 0\n",
    "num_workers = 0\n",
    "max_epochs = 10\n",
    "batch_size = 32\n",
    "splitter = 10\n",
    "device = \"gpu\"  # Set to 'gpu' for Kaggle GPU compatibility\n",
    "number_clients = 10\n",
    "save_results = \"results/FL/\"\n",
    "matrix_path = \"confusion_matrix.png\"\n",
    "roc_path = \"roc.png\"\n",
    "model_save = \"cifar_FHE.pt\"\n",
    "min_fit_clients = 10\n",
    "min_avail_clients = 10\n",
    "min_eval_clients = 10\n",
    "rounds = 20\n",
    "frac_fit = 1.0\n",
    "frac_eval = 0.5\n",
    "lr = 1e-3\n",
    "path_public_key = \"server_key.pkl\"\n",
    "\n",
    "DEVICE = torch.device(choice_device(device))\n",
    "CLASSES = classes_string(dataset)\n",
    "\n",
    "# Homomorphic encryption setup\n",
    "secret_path = \"secret.pkl\"\n",
    "public_path = path_public_key\n",
    "if os.path.exists(secret_path):\n",
    "    with open(secret_path, \"rb\") as f:\n",
    "        query = pickle.load(f)\n",
    "    context_client = ts.context_from(query[\"contexte\"])\n",
    "else:\n",
    "    context_client = context()\n",
    "    with open(secret_path, \"wb\") as f:\n",
    "        pickle.dump({\"contexte\": context_client.serialize(save_secret_key=True)}, f)\n",
    "    with open(public_path, \"wb\") as f:\n",
    "        pickle.dump({\"contexte\": context_client.serialize()}, f)\n",
    "context_server = ts.context_from(read_query(public_path)[1])\n",
    "\n",
    "# Load datasets\n",
    "trainloaders, valloaders, testloader = load_datasets(\n",
    "    num_clients=number_clients,\n",
    "    batch_size=batch_size,\n",
    "    resize=True,\n",
    "    seed=seed,\n",
    "    num_workers=num_workers,\n",
    "    splitter=splitter,\n",
    "    dataset=dataset,\n",
    "    data_path=data_path,\n",
    "    data_path_val=None,\n",
    ")\n",
    "\n",
    "# Define the model architecture\n",
    "n_qubits = 6\n",
    "n_layers = 6\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits)}\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(inputs, weights):\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # output: 64 x 16 x 16\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # output: 128 x 8 x 8\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # output: 256 x 4 x 4\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_qubits),\n",
    "        )\n",
    "        self.qnn = qml.qnn.TorchLayer(quantum_net, weight_shapes)\n",
    "        self.fc4 = nn.Linear(n_qubits, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.network(x)\n",
    "        x = self.qnn(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af283e16",
   "metadata": {},
   "source": [
    "# Main Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed37651",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = Net(num_classes=len(CLASSES)).to(DEVICE)\n",
    "initial_params = get_parameters2(global_model, context_server)\n",
    "global_serialized_params = serialize_parameters(initial_params)\n",
    "\n",
    "\n",
    "# Client training function (unchanged)\n",
    "def client_train(cid, serialized_global_params, local_epochs=max_epochs, lr=lr):\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    local_model = Net(num_classes=len(CLASSES)).to(DEVICE)\n",
    "    params = deserialize_parameters(serialized_global_params, context_client)\n",
    "    set_parameters(local_model, params, context_client)\n",
    "    optimizer = torch.optim.Adam(local_model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    results = train(\n",
    "        local_model,\n",
    "        trainloader,\n",
    "        valloader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        epochs=local_epochs,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    if save_results:\n",
    "        save_graphs(save_results, local_epochs, results, f\"_Client {cid}\")\n",
    "    updated_params = get_parameters2(local_model, context_client)\n",
    "    serialized_updated_params = serialize_parameters(updated_params)\n",
    "    num_examples = len(trainloader.dataset)\n",
    "    return serialized_updated_params, num_examples\n",
    "\n",
    "\n",
    "# Federated learning simulation with PySyft (updated to include testing)\n",
    "print(f\"Training on {DEVICE}\")\n",
    "start_simulation = time.time()\n",
    "\n",
    "# List to store test results\n",
    "test_accuracies = []\n",
    "test_losses = []\n",
    "\n",
    "for round_num in range(rounds):\n",
    "    client_updates = []\n",
    "    # Simulate client training\n",
    "    for cid in range(number_clients):\n",
    "        print(f\"[Client {cid}, round {round_num + 1}] training\")\n",
    "        serialized_updated_params, num_examples = client_train(\n",
    "            str(cid), global_serialized_params\n",
    "        )\n",
    "        client_updates.append((serialized_updated_params, num_examples))\n",
    "\n",
    "    # Aggregate updates\n",
    "    global_serialized_params = aggregate_serialized(client_updates, context_server)\n",
    "\n",
    "    # Test the global model after aggregation\n",
    "    aggregated_params = deserialize_parameters(global_serialized_params, context_client)\n",
    "    set_parameters(global_model, aggregated_params, context_client)\n",
    "    test_loss, test_acc, _, _, _ = test(\n",
    "        global_model, testloader, torch.nn.CrossEntropyLoss(), DEVICE\n",
    "    )\n",
    "    test_accuracies.append(test_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    print(\n",
    "        f\"Round {round_num + 1} Test Accuracy: {test_acc:.2f}%, Test Loss: {test_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Remove the re-encryption step - directly use aggregated encrypted parameters\n",
    "    print(f\"Round {round_num + 1} completed\")\n",
    "\n",
    "print(\n",
    "    f\"Federated learning completed. Simulation Time = {time.time() - start_simulation} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd2e10",
   "metadata": {},
   "source": [
    "# Save the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42291edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    os.makedirs(save_results, exist_ok=True)\n",
    "    torch.save({\"model_state_dict\": global_model.state_dict()}, model_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
