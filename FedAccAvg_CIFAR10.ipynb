{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd6cf08",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a074d612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T19:27:23.444082Z",
     "iopub.status.busy": "2025-05-06T19:27:23.443781Z",
     "iopub.status.idle": "2025-05-06T19:28:32.496760Z",
     "shell.execute_reply": "2025-05-06T19:28:32.495990Z"
    },
    "papermill": {
     "duration": 69.057822,
     "end_time": "2025-05-06T19:28:32.498397",
     "exception": false,
     "start_time": "2025-05-06T19:27:23.440575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tenseal syft pennylane\n",
    "!pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6201ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T19:28:32.529749Z",
     "iopub.status.busy": "2025-05-06T19:28:32.529335Z",
     "iopub.status.idle": "2025-05-06T22:42:04.053965Z",
     "shell.execute_reply": "2025-05-06T22:42:04.053088Z"
    },
    "papermill": {
     "duration": 11611.541114,
     "end_time": "2025-05-06T22:42:04.055311",
     "exception": false,
     "start_time": "2025-05-06T19:28:32.514197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "import syft as sy\n",
    "import pickle\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Dict, Optional, Callable, Union, cast\n",
    "import tenseal as ts\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import syft as sy\n",
    "from logging import WARNING\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0bad71",
   "metadata": {},
   "source": [
    "# Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c729a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choice_device(device):\n",
    "    if torch.cuda.is_available() and device != \"cpu\":\n",
    "        device = \"cuda:0\"\n",
    "    elif (\n",
    "        torch.backends.mps.is_available()\n",
    "        and torch.backends.mps.is_built()\n",
    "        and device != \"cpu\"\n",
    "    ):\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    return device\n",
    "\n",
    "\n",
    "def classes_string(name_dataset):\n",
    "    if name_dataset == \"cifar\":\n",
    "        return (\n",
    "            \"plane\",\n",
    "            \"car\",\n",
    "            \"bird\",\n",
    "            \"cat\",\n",
    "            \"deer\",\n",
    "            \"dog\",\n",
    "            \"frog\",\n",
    "            \"horse\",\n",
    "            \"ship\",\n",
    "            \"truck\",\n",
    "        )\n",
    "    elif name_dataset == \"MRI\":\n",
    "        return (\"glioma\", \"meningioma\", \"notumor\", \"pituitary\")\n",
    "    else:\n",
    "        print(\"Warning: unspecified dataset\")\n",
    "        return ()\n",
    "\n",
    "\n",
    "def save_matrix(y_true, y_pred, path, classes):\n",
    "    y_true_mapped = [classes[label] for label in y_true]\n",
    "    y_pred_mapped = [classes[label] for label in y_pred]\n",
    "    cf_matrix_normalized = confusion_matrix(\n",
    "        y_true_mapped, y_pred_mapped, labels=classes, normalize=\"all\"\n",
    "    )\n",
    "    cf_matrix_round = np.round(cf_matrix_normalized, 2)\n",
    "    df_cm = pd.DataFrame(\n",
    "        cf_matrix_round, index=[i for i in classes], columns=[i for i in classes]\n",
    "    )\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.xlabel(\"Predicted label\", fontsize=13)\n",
    "    plt.ylabel(\"True label\", fontsize=13)\n",
    "    plt.title(\"Confusion Matrix\", fontsize=15)\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_roc(targets, y_proba, path, nbr_classes):\n",
    "    y_true = np.zeros(shape=(len(targets), nbr_classes))\n",
    "    for i in range(len(targets)):\n",
    "        y_true[i, targets[i]] = 1\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(nbr_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(nbr_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(nbr_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= nbr_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    plt.figure()\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=f\"micro-average ROC curve (area = {roc_auc['micro']:.2f})\",\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "    plt.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=f\"macro-average ROC curve (area = {roc_auc['macro']:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "    lw = 2\n",
    "    for i in range(nbr_classes):\n",
    "        plt.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            lw=lw,\n",
    "            label=f\"ROC curve of class {i} (area = {roc_auc[i]:.2f})\",\n",
    "        )\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=lw, label=\"Worst case\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic (ROC) Curve OvR\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_graphs(path_save, local_epoch, results, end_file=\"\"):\n",
    "    os.makedirs(path_save, exist_ok=True)\n",
    "    print(\"Saving graphs in \", path_save)\n",
    "    plot_graph(\n",
    "        [[*range(local_epoch)]] * 2,\n",
    "        [results[\"train_acc\"], results[\"val_acc\"]],\n",
    "        \"Epochs\",\n",
    "        \"Accuracy (%)\",\n",
    "        [\"Training accuracy\", \"Validation accuracy\"],\n",
    "        \"Accuracy curves\",\n",
    "        path_save + \"Accuracy_curves\" + end_file,\n",
    "    )\n",
    "    plot_graph(\n",
    "        [[*range(local_epoch)]] * 2,\n",
    "        [results[\"train_loss\"], results[\"val_loss\"]],\n",
    "        \"Epochs\",\n",
    "        \"Loss\",\n",
    "        [\"Training loss\", \"Validation loss\"],\n",
    "        \"Loss curves\",\n",
    "        path_save + \"Loss_curves\" + end_file,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_graph(\n",
    "    list_xplot, list_yplot, x_label, y_label, curve_labels, title, path=None\n",
    "):\n",
    "    lw = 2\n",
    "    plt.figure()\n",
    "    for i in range(len(curve_labels)):\n",
    "        plt.plot(list_xplot[i], list_yplot[i], lw=lw, label=curve_labels[i])\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    if curve_labels:\n",
    "        plt.legend(loc=\"lower right\")\n",
    "    if path:\n",
    "        plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def get_parameters2(net, context_client=None) -> List[np.ndarray]:\n",
    "    if context_client:\n",
    "        encrypted_tensor = crypte(net.state_dict(), context_client)\n",
    "        return [layer.get_weight() for layer in encrypted_tensor]\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray], context_client=None):\n",
    "    state_dict = net.state_dict()\n",
    "    params_dict = zip(state_dict.keys(), parameters)\n",
    "    if context_client:\n",
    "        secret_key = context_client.secret_key()\n",
    "        dico = {k: deserialized_layer(k, v, context_client) for k, v in params_dict}\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in dico.items():\n",
    "            if isinstance(v, CryptedLayer):\n",
    "                decrypted = v.decrypt(secret_key)\n",
    "                shape = state_dict[k].shape\n",
    "                new_state_dict[k] = torch.Tensor(np.array(decrypted).reshape(shape))\n",
    "            else:\n",
    "                new_state_dict[k] = torch.Tensor(v.get_weight())\n",
    "    else:\n",
    "        new_state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(new_state_dict, strict=True)\n",
    "    print(\"Updated model parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af865a",
   "metadata": {},
   "source": [
    "# Security-related classes and functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2abbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, name_layer, weight):\n",
    "        self.name = name_layer\n",
    "        self.weight_array = weight\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def get_weight(self):\n",
    "        return self.weight_array\n",
    "\n",
    "    def __add__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        return Layer(self.name, self.weight_array + weights)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        return Layer(self.name, self.weight_array - weights)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        return Layer(self.name, self.weight_array * weights)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        weights = self.weight_array * (1 / weights)\n",
    "        return Layer(self.name, weights)\n",
    "\n",
    "    def __len__(self):\n",
    "        somme = 1\n",
    "        for elem in self.weight_array.shape:\n",
    "            somme *= elem\n",
    "        return somme\n",
    "\n",
    "    def shape(self):\n",
    "        return self.weight_array.shape\n",
    "\n",
    "    def sum(self, axis=0):\n",
    "        return Layer(f\"sum_{self.name}\", self.weight_array.sum(axis=axis))\n",
    "\n",
    "    def mean(self, axis=0):\n",
    "        weights = self.weight_array.sum(axis=axis) * (1 / self.weight_array.shape[axis])\n",
    "        return Layer(f\"sum_{self.name}\", weights)\n",
    "\n",
    "    def decrypt(self, sk=None):\n",
    "        return self.weight_array.tolist()\n",
    "\n",
    "    def serialize(self):\n",
    "        return {self.name: self.weight_array}\n",
    "\n",
    "\n",
    "class CryptedLayer(Layer):\n",
    "    def __init__(self, name_layer, weight, contexte=None):\n",
    "        super(CryptedLayer, self).__init__(name_layer, weight)\n",
    "        if isinstance(weight, (ts.tensors.CKKSTensor, bytes)):\n",
    "            self.weight_array = weight\n",
    "        else:\n",
    "            self.weight_array = ts.ckks_tensor(contexte, weight.cpu().detach().numpy())\n",
    "\n",
    "    def __add__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "        return CryptedLayer(self.name, self.weight_array + weights)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "        return CryptedLayer(self.name, self.weight_array - weights)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "        return CryptedLayer(self.name, self.weight_array * weights)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        try:\n",
    "            weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "            weights = self.weight_array * (1 / weights)\n",
    "        except:\n",
    "            print(\"Error: division operator not supported by SEAL\")\n",
    "            weights = []\n",
    "        return CryptedLayer(self.name, weights)\n",
    "\n",
    "    def shape(self):\n",
    "        return self.weight_array.shape\n",
    "\n",
    "    def sum(self, axis=0):\n",
    "        return CryptedLayer(f\"sum_{self.name}\", self.weight_array.sum(axis=axis))\n",
    "\n",
    "    def mean(self, axis=0):\n",
    "        weights = self.weight_array.sum(axis=axis) * (1 / self.weight_array.shape[axis])\n",
    "        return CryptedLayer(f\"sum_{self.name}\", weights)\n",
    "\n",
    "    def decrypt(self, sk=None):\n",
    "        return (\n",
    "            self.weight_array.decrypt(sk).tolist()\n",
    "            if sk\n",
    "            else self.weight_array.decrypt().tolist()\n",
    "        )\n",
    "\n",
    "    def serialize(self):\n",
    "        return {self.name: self.weight_array.serialize()}\n",
    "\n",
    "\n",
    "def context():\n",
    "    cont = ts.context(\n",
    "        ts.SCHEME_TYPE.CKKS,\n",
    "        poly_modulus_degree=8192,\n",
    "        coeff_mod_bit_sizes=[60, 40, 40, 60],\n",
    "    )\n",
    "    cont.generate_galois_keys()\n",
    "    cont.global_scale = 2**40\n",
    "    return cont\n",
    "\n",
    "\n",
    "def crypte(client_w, context_c):\n",
    "    encrypted = []\n",
    "    for name_layer, weight_array in client_w.items():\n",
    "        if name_layer == \"fc4.weight\":\n",
    "            encrypted.append(CryptedLayer(name_layer, weight_array, context_c))\n",
    "        else:\n",
    "            encrypted.append(Layer(name_layer, weight_array))\n",
    "    return encrypted\n",
    "\n",
    "\n",
    "def read_query(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            query_str = pickle.load(file)\n",
    "        contexte = query_str[\"contexte\"]\n",
    "        del query_str[\"contexte\"]\n",
    "        return query_str, contexte\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def write_query(file_path, client_query):\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        encode_str = pickle.dumps(client_query)\n",
    "        file.write(encode_str)\n",
    "\n",
    "\n",
    "def deserialized_layer(name_layer, weight_array, ctx):\n",
    "    if isinstance(weight_array, bytes):\n",
    "        return CryptedLayer(name_layer, ts.ckks_tensor_from(ctx, weight_array), ctx)\n",
    "    elif isinstance(weight_array, ts.tensors.CKKSTensor):\n",
    "        return CryptedLayer(name_layer, weight_array, ctx)\n",
    "    else:\n",
    "        return Layer(name_layer, weight_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cc09a",
   "metadata": {},
   "source": [
    "# Data setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_DICT = {\n",
    "    \"cifar\": dict(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    \"MRI\": dict(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "}\n",
    "\n",
    "\n",
    "def split_data_client(dataset, num_clients, seed):\n",
    "    partition_size = len(dataset) // num_clients\n",
    "    lengths = [partition_size] * (num_clients - 1)\n",
    "    lengths += [len(dataset) - sum(lengths)]\n",
    "    ds = random_split(dataset, lengths, torch.Generator().manual_seed(seed))\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    num_clients: int,\n",
    "    batch_size: int,\n",
    "    resize: int,\n",
    "    seed: int,\n",
    "    num_workers: int,\n",
    "    splitter=10,\n",
    "    dataset=\"cifar\",\n",
    "    data_path=\"./data/\",\n",
    "    data_path_val=\"\",\n",
    "):\n",
    "    list_transforms = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**NORMALIZE_DICT[dataset]),\n",
    "    ]\n",
    "    if dataset != \"cifar\" and resize is not None:\n",
    "        list_transforms = [transforms.Resize((resize, resize))] + list_transforms\n",
    "    transformer = transforms.Compose(list_transforms)\n",
    "    if dataset == \"cifar\":\n",
    "        trainset = datasets.CIFAR10(\n",
    "            data_path + dataset, train=True, download=True, transform=transformer\n",
    "        )\n",
    "        testset = datasets.CIFAR10(\n",
    "            data_path + dataset, train=False, download=True, transform=transformer\n",
    "        )\n",
    "    else:\n",
    "        trainset = datasets.ImageFolder(\n",
    "            data_path + dataset + \"/Training\", transform=transformer\n",
    "        )\n",
    "        testset = datasets.ImageFolder(\n",
    "            data_path + dataset + \"/Testing\", transform=transformer\n",
    "        )\n",
    "\n",
    "    datasets_train = split_data_client(trainset, num_clients, seed)\n",
    "    if data_path_val:\n",
    "        valset = datasets.ImageFolder(data_path_val, transform=transformer)\n",
    "        datasets_val = split_data_client(valset, num_clients, seed)\n",
    "    else:\n",
    "        datasets_val = None\n",
    "\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for i in range(num_clients):\n",
    "        if data_path_val:\n",
    "            trainloaders.append(\n",
    "                DataLoader(datasets_train[i], batch_size=batch_size, shuffle=True)\n",
    "            )\n",
    "            valloaders.append(DataLoader(datasets_val[i], batch_size=batch_size))\n",
    "        else:\n",
    "            len_val = int(len(datasets_train[i]) * splitter / 100)\n",
    "            len_train = len(datasets_train[i]) - len_val\n",
    "            lengths = [len_train, len_val]\n",
    "            ds_train, ds_val = random_split(\n",
    "                datasets_train[i], lengths, torch.Generator().manual_seed(seed)\n",
    "            )\n",
    "            trainloaders.append(\n",
    "                DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "            )\n",
    "            valloaders.append(DataLoader(ds_val, batch_size=batch_size))\n",
    "    testloader = DataLoader(testset, batch_size=batch_size)\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f1689",
   "metadata": {},
   "source": [
    "# Training and testing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: Union[torch.nn.Module, Tuple],\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_proba = []\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images)\n",
    "            probas_output = softmax(output)\n",
    "            y_proba.extend(probas_output.detach().cpu().numpy())\n",
    "            loss = loss_fn(output, labels)\n",
    "            test_loss += loss.item()\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            y_pred.extend(preds)\n",
    "            acc = (preds == labels).mean()\n",
    "            test_acc += acc\n",
    "    y_proba = np.array(y_proba)\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc * 100, y_pred, y_true, y_proba\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: Union[torch.nn.Module, Tuple],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred_class = torch.argmax(torch.softmax(output, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == labels).sum().item() / len(output)\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc * 100\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Union[torch.nn.Module, Tuple],\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, List]:\n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(\n",
    "            model, train_dataloader, loss_fn, optimizer, device\n",
    "        )\n",
    "        val_loss, val_acc, *_ = test(model, test_dataloader, loss_fn, device)\n",
    "        print(\n",
    "            f\"\\tTrain Epoch: {epoch + 1} \\tTrain_loss: {train_loss:.4f} | Train_acc: {train_acc:.4f} % | \"\n",
    "            f\"Validation_loss: {val_loss:.4f} | Validation_acc: {val_acc:.4f} %\"\n",
    "        )\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "    return results\n",
    "\n",
    "\n",
    "def serialize_ndarray(ndarray):\n",
    "    if isinstance(ndarray, ts.tensors.CKKSTensor):\n",
    "        return ndarray.serialize()\n",
    "    elif isinstance(ndarray, torch.Tensor):\n",
    "        return serialize_ndarray(ndarray.cpu().detach().numpy())\n",
    "    else:\n",
    "        bytes_io = BytesIO()\n",
    "        np.save(bytes_io, ndarray, allow_pickle=False)\n",
    "        return bytes_io.getvalue()\n",
    "\n",
    "\n",
    "def deserialize_ndarray(tensor, context):\n",
    "    try:\n",
    "        return ts.ckks_tensor_from(context, tensor)\n",
    "    except:\n",
    "        bytes_io = BytesIO(tensor)\n",
    "        return np.load(bytes_io, allow_pickle=False)\n",
    "\n",
    "\n",
    "def serialize_parameters(parameters):\n",
    "    return [serialize_ndarray(param) for param in parameters]\n",
    "\n",
    "\n",
    "def deserialize_parameters(serialized_params, context):\n",
    "    return [deserialize_ndarray(param, context) for param in serialized_params]\n",
    "\n",
    "\n",
    "def privatize_accuracy(true_acc: float, N: int, ε=1.0):\n",
    "    sensitivity = 1.0 / N\n",
    "    noise = np.random.laplace(0, sensitivity / ε)\n",
    "    return np.clip(true_acc + noise, 0, 1)\n",
    "\n",
    "\n",
    "def accuracy_weights(accuracies: List[float], τ=0.5) -> List[float]:\n",
    "    scaled_acc = [a / τ for a in accuracies]\n",
    "    max_scaled = max(scaled_acc)\n",
    "    exp_acc = [np.exp(a - max_scaled) for a in scaled_acc]\n",
    "    sum_exp = sum(exp_acc)\n",
    "    return [e / sum_exp for e in exp_acc]\n",
    "\n",
    "\n",
    "def compute_difference_norm(new_param, prev_param, context):\n",
    "    if isinstance(new_param, ts.tensors.CKKSTensor):\n",
    "        new_dec = new_param.decrypt(context.secret_key()).tolist()\n",
    "        prev_dec = prev_param.decrypt(context.secret_key()).tolist()\n",
    "        diff = np.array(new_dec) - np.array(prev_dec)\n",
    "    else:\n",
    "        if isinstance(new_param, torch.Tensor):\n",
    "            new_param = new_param.cpu().numpy()\n",
    "        if isinstance(prev_param, torch.Tensor):\n",
    "            prev_param = prev_param.cpu().numpy()\n",
    "        diff = new_param - prev_param\n",
    "    return np.linalg.norm(diff)\n",
    "\n",
    "\n",
    "def aggregate_serialized(results, context, τ=0.5):\n",
    "    accuracies = [dp_acc for _, dp_acc in results]\n",
    "    weights = accuracy_weights(accuracies, τ)\n",
    "\n",
    "    weights_results = [\n",
    "        (deserialize_parameters(serialized_params, context), w)\n",
    "        for (serialized_params, _), w in zip(results, weights)\n",
    "    ]\n",
    "\n",
    "    aggregated_params = []\n",
    "    for layer_idx in range(len(weights_results[0][0])):\n",
    "        layer_updates = [weights[layer_idx] for weights, _ in weights_results]\n",
    "        if isinstance(layer_updates[0], ts.tensors.CKKSTensor):\n",
    "            weighted_sum = sum([layer * w for layer, w in zip(layer_updates, weights)])\n",
    "        else:\n",
    "            weighted_sum = sum([layer * w for layer, w in zip(layer_updates, weights)])\n",
    "        aggregated_params.append(weighted_sum)\n",
    "    return serialize_parameters(aggregated_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6979e8",
   "metadata": {},
   "source": [
    "# Main experiment setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fda0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "he = True\n",
    "data_path = \"data/\"\n",
    "dataset = \"cifar\"\n",
    "yaml_path = \"./results/FL/results.yml\"\n",
    "seed = 42\n",
    "num_workers = 0\n",
    "max_epochs = 10\n",
    "batch_size = 32\n",
    "splitter = 10\n",
    "device = \"gpu\"\n",
    "number_clients = 10\n",
    "save_results = \"results/FL/\"\n",
    "matrix_path = \"confusion_matrix.png\"\n",
    "roc_path = \"roc.png\"\n",
    "model_save = \"cifar_FHE.pt\"\n",
    "min_fit_clients = 2\n",
    "min_avail_clients = 2\n",
    "min_eval_clients = 2\n",
    "rounds = 20\n",
    "frac_fit = 1.0\n",
    "frac_eval = 0.5\n",
    "lr = 1e-3\n",
    "path_public_key = \"server_key.pkl\"\n",
    "\n",
    "DEVICE = torch.device(choice_device(device))\n",
    "CLASSES = classes_string(dataset)\n",
    "\n",
    "n_qubits = 4\n",
    "n_layers = 2\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}  # For StronglyEntanglingLayers\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(inputs, weights):\n",
    "    # Automatically pad with zeros and normalize\n",
    "    qml.AmplitudeEmbedding(\n",
    "        features=inputs, wires=range(n_qubits), pad_with=0.0, normalize=True\n",
    "    )\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2**n_qubits),\n",
    "        )\n",
    "        self.qnn = qml.qnn.TorchLayer(quantum_net, weight_shapes)\n",
    "        self.fc4 = nn.Linear(n_qubits, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.network(x)\n",
    "        x = self.qnn(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9590ad",
   "metadata": {},
   "source": [
    "# Main Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_path = \"secret.pkl\"\n",
    "public_path = path_public_key\n",
    "if os.path.exists(secret_path):\n",
    "    with open(secret_path, \"rb\") as f:\n",
    "        query = pickle.load(f)\n",
    "    context_client = ts.context_from(query[\"contexte\"])\n",
    "else:\n",
    "    context_client = context()\n",
    "    with open(secret_path, \"wb\") as f:\n",
    "        pickle.dump({\"contexte\": context_client.serialize(save_secret_key=True)}, f)\n",
    "    with open(public_path, \"wb\") as f:\n",
    "        pickle.dump({\"contexte\": context_client.serialize()}, f)\n",
    "context_server = ts.context_from(read_query(public_path)[1])\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(\n",
    "    number_clients, batch_size, resize=None, seed=seed, num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize global model\n",
    "global_model = Net(num_classes=len(CLASSES)).to(DEVICE)\n",
    "initial_params = get_parameters2(global_model, context_server)\n",
    "global_serialized_params = serialize_parameters(initial_params)\n",
    "\n",
    "\n",
    "# Client training function\n",
    "def client_train(cid, serialized_global_params, local_epochs=max_epochs, lr=lr):\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    local_model = Net(num_classes=len(CLASSES)).to(DEVICE)\n",
    "    params = deserialize_parameters(serialized_global_params, context_client)\n",
    "    set_parameters(local_model, params, context_client)\n",
    "    optimizer = torch.optim.Adam(local_model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    results = train(\n",
    "        local_model,\n",
    "        trainloader,\n",
    "        valloader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        epochs=local_epochs,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    if save_results:\n",
    "        save_graphs(save_results, local_epochs, results, f\"_Client {cid}\")\n",
    "    updated_params = get_parameters2(local_model, context_client)\n",
    "    serialized_updated_params = serialize_parameters(updated_params)\n",
    "    num_examples = len(trainloader.dataset)\n",
    "    val_acc = results[\"val_acc\"][-1] / 100\n",
    "    N_val = len(valloader.dataset)\n",
    "    dp_acc = privatize_accuracy(val_acc, N_val)\n",
    "    print(f\"[Client {cid}] True Acc: {val_acc:.4f}, DP Acc: {dp_acc:.4f}\")\n",
    "    return serialized_updated_params, num_examples, dp_acc\n",
    "\n",
    "\n",
    "# Federated learning simulation\n",
    "print(f\"Training on {DEVICE}\")\n",
    "start_simulation = time.time()\n",
    "\n",
    "layer_names = list(global_model.state_dict().keys())\n",
    "quantum_layer_indices = [i for i, name in enumerate(layer_names) if \"qnn\" in name]\n",
    "previous_aggregated_params = initial_params\n",
    "importance_history = [1.0] * len(initial_params)\n",
    "α = 0.9\n",
    "threshold = 0.001\n",
    "\n",
    "for round_num in range(rounds):\n",
    "    client_updates = []\n",
    "    for cid in range(number_clients):\n",
    "        print(f\"[Client {cid}, round {round_num + 1}] training\")\n",
    "        serialized_updated_params, _, dp_acc = client_train(\n",
    "            str(cid), global_serialized_params\n",
    "        )\n",
    "        client_updates.append((serialized_updated_params, dp_acc))\n",
    "\n",
    "    accuracies = [dp_acc for _, dp_acc in client_updates]\n",
    "    weights = accuracy_weights(accuracies)\n",
    "    print(\n",
    "        f\"[Round {round_num + 1}] Client DP Accuracies: {accuracies}, Weights: {weights}\"\n",
    "    )\n",
    "    global_serialized_params = aggregate_serialized(client_updates, context_server)\n",
    "    new_aggregated_params = deserialize_parameters(\n",
    "        global_serialized_params, context_server\n",
    "    )\n",
    "\n",
    "    differences = [\n",
    "        compute_difference_norm(new, prev, context_client)\n",
    "        for new, prev in zip(new_aggregated_params, previous_aggregated_params)\n",
    "    ]\n",
    "    importance_history = [\n",
    "        α * imp + (1 - α) * diff for imp, diff in zip(importance_history, differences)\n",
    "    ]\n",
    "    print(f\"[Round {round_num + 1}] Importance History: {importance_history}\")\n",
    "\n",
    "    frozen_layers = []\n",
    "    for i, imp in enumerate(importance_history):\n",
    "        if imp < threshold and i not in quantum_layer_indices:\n",
    "            new_aggregated_params[i] = previous_aggregated_params[i]\n",
    "            frozen_layers.append(i)\n",
    "    print(f\"[Round {round_num + 1}] Frozen Layers: {frozen_layers}\")\n",
    "\n",
    "    # Update global model with new aggregated parameters and evaluate\n",
    "    set_parameters(global_model, new_aggregated_params, context_client)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    test_loss, test_acc, _, _, _ = test(global_model, testloader, criterion, DEVICE)\n",
    "    print(\n",
    "        f\"[Round {round_num + 1}] Global Model Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f} %\"\n",
    "    )\n",
    "\n",
    "    global_serialized_params = serialize_parameters(new_aggregated_params)\n",
    "    previous_aggregated_params = new_aggregated_params.copy()\n",
    "    print(f\"Round {round_num + 1} completed\")\n",
    "\n",
    "print(\n",
    "    f\"Federated learning completed. Simulation Time = {time.time() - start_simulation} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59ea77",
   "metadata": {},
   "source": [
    "# Save the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    os.makedirs(save_results, exist_ok=True)\n",
    "    torch.save({\"model_state_dict\": global_model.state_dict()}, model_save)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1608934,
     "sourceId": 2645886,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11687.614668,
   "end_time": "2025-05-06T22:42:06.982397",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-06T19:27:19.367729",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
