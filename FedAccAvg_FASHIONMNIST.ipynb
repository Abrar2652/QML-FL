{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ec7a05",
   "metadata": {
    "papermill": {
     "duration": 0.00368,
     "end_time": "2025-05-18T15:25:30.752826",
     "exception": false,
     "start_time": "2025-05-18T15:25:30.749146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install and Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6045995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T15:25:30.759412Z",
     "iopub.status.busy": "2025-05-18T15:25:30.759194Z",
     "iopub.status.idle": "2025-05-18T15:26:37.774952Z",
     "shell.execute_reply": "2025-05-18T15:26:37.774122Z"
    },
    "papermill": {
     "duration": 67.020485,
     "end_time": "2025-05-18T15:26:37.776442",
     "exception": false,
     "start_time": "2025-05-18T15:25:30.755957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tenseal syft pennylane\n",
    "!pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b44c63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T15:26:37.805285Z",
     "iopub.status.busy": "2025-05-18T15:26:37.805025Z",
     "iopub.status.idle": "2025-05-18T15:27:08.865510Z",
     "shell.execute_reply": "2025-05-18T15:27:08.864916Z"
    },
    "papermill": {
     "duration": 31.07617,
     "end_time": "2025-05-18T15:27:08.866902",
     "exception": false,
     "start_time": "2025-05-18T15:26:37.790732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "import syft as sy\n",
    "import pickle\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Dict, Optional, Callable, Union, cast\n",
    "import tenseal as ts\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import syft as sy\n",
    "from logging import WARNING\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8564c5f",
   "metadata": {
    "papermill": {
     "duration": 0.013942,
     "end_time": "2025-05-18T15:27:08.895413",
     "exception": false,
     "start_time": "2025-05-18T15:27:08.881471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784356c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T15:27:08.924427Z",
     "iopub.status.busy": "2025-05-18T15:27:08.923887Z",
     "iopub.status.idle": "2025-05-18T15:27:08.942217Z",
     "shell.execute_reply": "2025-05-18T15:27:08.941705Z"
    },
    "papermill": {
     "duration": 0.034078,
     "end_time": "2025-05-18T15:27:08.943148",
     "exception": false,
     "start_time": "2025-05-18T15:27:08.909070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def choice_device(device):\n",
    "    if torch.cuda.is_available() and device != \"cpu\":\n",
    "        device = \"cuda:0\"\n",
    "    elif (\n",
    "        torch.backends.mps.is_available()\n",
    "        and torch.backends.mps.is_built()\n",
    "        and device != \"cpu\"\n",
    "    ):\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    return device\n",
    "\n",
    "\n",
    "def classes_string(name_dataset):\n",
    "    if name_dataset == \"cifar\":\n",
    "        return (\n",
    "            \"plane\",\n",
    "            \"car\",\n",
    "            \"bird\",\n",
    "            \"cat\",\n",
    "            \"deer\",\n",
    "            \"dog\",\n",
    "            \"frog\",\n",
    "            \"horse\",\n",
    "            \"ship\",\n",
    "            \"truck\",\n",
    "        )\n",
    "    elif name_dataset == \"svhn\":\n",
    "        return tuple(str(i) for i in range(10))  # SVHN has 10 classes (digits 0-9)\n",
    "    elif name_dataset == \"caltech101\":\n",
    "        return tuple([f\"class_{i}\" for i in range(101)])  # Caltech101 has 101 classes\n",
    "    elif name_dataset == \"stanfordcars\":\n",
    "        return tuple([f\"class_{i}\" for i in range(196)])  # StanfordCars has 196 classes\n",
    "    elif name_dataset == \"fashion_mnist\":\n",
    "        return (\n",
    "            \"T-shirt/top\",\n",
    "            \"Trouser\",\n",
    "            \"Pullover\",\n",
    "            \"Dress\",\n",
    "            \"Coat\",\n",
    "            \"Sandal\",\n",
    "            \"Shirt\",\n",
    "            \"Sneaker\",\n",
    "            \"Bag\",\n",
    "            \"Ankle boot\",\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {name_dataset}\")\n",
    "\n",
    "\n",
    "def save_matrix(y_true, y_pred, path, classes):\n",
    "    y_true_mapped = [classes[label] for label in y_true]\n",
    "    y_pred_mapped = [classes[label] for label in y_pred]\n",
    "    cf_matrix_normalized = confusion_matrix(\n",
    "        y_true_mapped, y_pred_mapped, labels=classes, normalize=\"all\"\n",
    "    )\n",
    "    cf_matrix_round = np.round(cf_matrix_normalized, 2)\n",
    "    df_cm = pd.DataFrame(\n",
    "        cf_matrix_round, index=[i for i in classes], columns=[i for i in classes]\n",
    "    )\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.xlabel(\"Predicted label\", fontsize=13)\n",
    "    plt.ylabel(\"True label\", fontsize=13)\n",
    "    plt.title(\"Confusion Matrix\", fontsize=15)\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_roc(targets, y_proba, path, nbr_classes):\n",
    "    y_true = np.zeros(shape=(len(targets), nbr_classes))\n",
    "    for i in range(len(targets)):\n",
    "        y_true[i, targets[i]] = 1\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(nbr_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(nbr_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(nbr_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= nbr_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    plt.figure()\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=f\"micro-average ROC curve (area = {roc_auc['micro']:.2f})\",\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "    plt.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=f\"macro-average ROC curve (area = {roc_auc['macro']:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "    lw = 2\n",
    "    for i in range(nbr_classes):\n",
    "        plt.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            lw=lw,\n",
    "            label=f\"ROC curve of class {i} (area = {roc_auc[i]:.2f})\",\n",
    "        )\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=lw, label=\"Worst case\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic (ROC) Curve OvR\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_graphs(path_save, local_epoch, results, end_file=\"\"):\n",
    "    os.makedirs(path_save, exist_ok=True)\n",
    "    print(\"Saving graphs in \", path_save)\n",
    "    plot_graph(\n",
    "        [[*range(local_epoch)]] * 2,\n",
    "        [results[\"train_acc\"], results[\"val_acc\"]],\n",
    "        \"Epochs\",\n",
    "        \"Accuracy (%)\",\n",
    "        [\"Training accuracy\", \"Validation accuracy\"],\n",
    "        \"Accuracy curves\",\n",
    "        path_save + \"Accuracy_curves\" + end_file,\n",
    "    )\n",
    "    plot_graph(\n",
    "        [[*range(local_epoch)]] * 2,\n",
    "        [results[\"train_loss\"], results[\"val_loss\"]],\n",
    "        \"Epochs\",\n",
    "        \"Loss\",\n",
    "        [\"Training loss\", \"Validation loss\"],\n",
    "        \"Loss curves\",\n",
    "        path_save + \"Loss_curves\" + end_file,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_graph(\n",
    "    list_xplot, list_yplot, x_label, y_label, curve_labels, title, path=None\n",
    "):\n",
    "    lw = 2\n",
    "    plt.figure()\n",
    "    for i in range(len(curve_labels)):\n",
    "        plt.plot(list_xplot[i], list_yplot[i], lw=lw, label=curve_labels[i])\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    if curve_labels:\n",
    "        plt.legend(loc=\"lower right\")\n",
    "    if path:\n",
    "        plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def get_parameters2(net, context_client=None) -> List[np.ndarray]:\n",
    "    if context_client:\n",
    "        encrypted_tensor = crypte(net.state_dict(), context_client)\n",
    "        return [layer.get_weight() for layer in encrypted_tensor]\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray], context_client=None):\n",
    "    state_dict = net.state_dict()\n",
    "    params_dict = zip(state_dict.keys(), parameters)\n",
    "    if context_client:\n",
    "        secret_key = context_client.secret_key()\n",
    "        dico = {k: deserialized_layer(k, v, context_client) for k, v in params_dict}\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in dico.items():\n",
    "            if isinstance(v, CryptedLayer):\n",
    "                decrypted = v.decrypt(secret_key)\n",
    "                shape = state_dict[k].shape\n",
    "                new_state_dict[k] = torch.Tensor(np.array(decrypted).reshape(shape))\n",
    "            else:\n",
    "                new_state_dict[k] = torch.Tensor(v.get_weight())\n",
    "    else:\n",
    "        new_state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(new_state_dict, strict=True)\n",
    "    print(\"Updated model parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce92c85",
   "metadata": {
    "papermill": {
     "duration": 0.013268,
     "end_time": "2025-05-18T15:27:08.970096",
     "exception": false,
     "start_time": "2025-05-18T15:27:08.956828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Security-related classes and functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d2964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T15:27:08.998327Z",
     "iopub.status.busy": "2025-05-18T15:27:08.998107Z",
     "iopub.status.idle": "2025-05-18T15:27:09.012986Z",
     "shell.execute_reply": "2025-05-18T15:27:09.012283Z"
    },
    "papermill": {
     "duration": 0.030401,
     "end_time": "2025-05-18T15:27:09.014086",
     "exception": false,
     "start_time": "2025-05-18T15:27:08.983685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, name_layer, weight):\n",
    "        self.name = name_layer\n",
    "        self.weight_array = weight\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def get_weight(self):\n",
    "        return self.weight_array\n",
    "\n",
    "    def __add__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        return Layer(self.name, self.weight_array + weights)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        return Layer(self.name, self.weight_array - weights)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        return Layer(self.name, self.weight_array * weights)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, Layer) else other\n",
    "        weights = self.weight_array * (1 / weights)\n",
    "        return Layer(self.name, weights)\n",
    "\n",
    "    def __len__(self):\n",
    "        somme = 1\n",
    "        for elem in self.weight_array.shape:\n",
    "            somme *= elem\n",
    "        return somme\n",
    "\n",
    "    def shape(self):\n",
    "        return self.weight_array.shape\n",
    "\n",
    "    def sum(self, axis=0):\n",
    "        return Layer(f\"sum_{self.name}\", self.weight_array.sum(axis=axis))\n",
    "\n",
    "    def mean(self, axis=0):\n",
    "        weights = self.weight_array.sum(axis=axis) * (1 / self.weight_array.shape[axis])\n",
    "        return Layer(f\"sum_{self.name}\", weights)\n",
    "\n",
    "    def decrypt(self, sk=None):\n",
    "        return self.weight_array.tolist()\n",
    "\n",
    "    def serialize(self):\n",
    "        return {self.name: self.weight_array}\n",
    "\n",
    "\n",
    "class CryptedLayer(Layer):\n",
    "    def __init__(self, name_layer, weight, contexte=None):\n",
    "        super(CryptedLayer, self).__init__(name_layer, weight)\n",
    "        if isinstance(weight, (ts.tensors.CKKSTensor, bytes)):\n",
    "            self.weight_array = weight\n",
    "        else:\n",
    "            self.weight_array = ts.ckks_tensor(contexte, weight.cpu().detach().numpy())\n",
    "\n",
    "    def __add__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "        return CryptedLayer(self.name, self.weight_array + weights)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "        return CryptedLayer(self.name, self.weight_array - weights)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "        return CryptedLayer(self.name, self.weight_array * weights)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        try:\n",
    "            weights = other.get_weight() if isinstance(other, CryptedLayer) else other\n",
    "            weights = self.weight_array * (1 / weights)\n",
    "        except:\n",
    "            print(\"Error: division operator not supported by SEAL\")\n",
    "            weights = []\n",
    "        return CryptedLayer(self.name, weights)\n",
    "\n",
    "    def shape(self):\n",
    "        return self.weight_array.shape\n",
    "\n",
    "    def sum(self, axis=0):\n",
    "        return CryptedLayer(f\"sum_{self.name}\", self.weight_array.sum(axis=axis))\n",
    "\n",
    "    def mean(self, axis=0):\n",
    "        weights = self.weight_array.sum(axis=axis) * (1 / self.weight_array.shape[axis])\n",
    "        return CryptedLayer(f\"sum_{self.name}\", weights)\n",
    "\n",
    "    def decrypt(self, sk=None):\n",
    "        return (\n",
    "            self.weight_array.decrypt(sk).tolist()\n",
    "            if sk\n",
    "            else self.weight_array.decrypt().tolist()\n",
    "        )\n",
    "\n",
    "    def serialize(self):\n",
    "        return {self.name: self.weight_array.serialize()}\n",
    "\n",
    "\n",
    "def context():\n",
    "    cont = ts.context(\n",
    "        ts.SCHEME_TYPE.CKKS,\n",
    "        poly_modulus_degree=8192,\n",
    "        coeff_mod_bit_sizes=[60, 40, 40, 60],\n",
    "    )\n",
    "    cont.generate_galois_keys()\n",
    "    cont.global_scale = 2**40\n",
    "    return cont\n",
    "\n",
    "\n",
    "def crypte(client_w, context_c):\n",
    "    encrypted = []\n",
    "    for name_layer, weight_array in client_w.items():\n",
    "        if name_layer == \"fc4.weight\":\n",
    "            encrypted.append(CryptedLayer(name_layer, weight_array, context_c))\n",
    "        else:\n",
    "            encrypted.append(Layer(name_layer, weight_array))\n",
    "    return encrypted\n",
    "\n",
    "\n",
    "def read_query(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            query_str = pickle.load(file)\n",
    "        contexte = query_str[\"contexte\"]\n",
    "        del query_str[\"contexte\"]\n",
    "        return query_str, contexte\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def write_query(file_path, client_query):\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        encode_str = pickle.dumps(client_query)\n",
    "        file.write(encode_str)\n",
    "\n",
    "\n",
    "def deserialized_layer(name_layer, weight_array, ctx):\n",
    "    if isinstance(weight_array, bytes):\n",
    "        return CryptedLayer(name_layer, ts.ckks_tensor_from(ctx, weight_array), ctx)\n",
    "    elif isinstance(weight_array, ts.tensors.CKKSTensor):\n",
    "        return CryptedLayer(name_layer, weight_array, ctx)\n",
    "    else:\n",
    "        return Layer(name_layer, weight_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7bcfd",
   "metadata": {
    "papermill": {
     "duration": 0.013346,
     "end_time": "2025-05-18T15:27:09.040968",
     "exception": false,
     "start_time": "2025-05-18T15:27:09.027622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97477cf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T15:27:09.069588Z",
     "iopub.status.busy": "2025-05-18T15:27:09.069100Z",
     "iopub.status.idle": "2025-05-18T15:27:09.080725Z",
     "shell.execute_reply": "2025-05-18T15:27:09.080183Z"
    },
    "papermill": {
     "duration": 0.027073,
     "end_time": "2025-05-18T15:27:09.081741",
     "exception": false,
     "start_time": "2025-05-18T15:27:09.054668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NORMALIZE_DICT = {\n",
    "    \"cifar\": dict(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    \"svhn\": dict(mean=(0.4377, 0.4438, 0.4728), std=(0.1980, 0.2010, 0.1970)),\n",
    "    \"caltech101\": dict(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    \"stanfordcars\": dict(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    \"fashion_mnist\": dict(mean=(0.2860,), std=(0.3530,)),  # Add Fashion-MNIST\n",
    "}\n",
    "\n",
    "\n",
    "def split_data_client(dataset, num_clients, seed):\n",
    "\n",
    "    partition_size = len(dataset) // num_clients\n",
    "\n",
    "    lengths = [partition_size] * (num_clients - 1)\n",
    "\n",
    "    lengths += [len(dataset) - sum(lengths)]\n",
    "\n",
    "    ds = random_split(dataset, lengths, torch.Generator().manual_seed(seed))\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    num_clients: int,\n",
    "    batch_size: int,\n",
    "    resize: int,\n",
    "    seed: int,\n",
    "    num_workers: int,\n",
    "    splitter=10,\n",
    "    dataset=\"cifar\",\n",
    "    data_path=\"./data/\",\n",
    "    data_path_val=\"\",\n",
    "):\n",
    "    list_transforms = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**NORMALIZE_DICT[dataset]),\n",
    "    ]\n",
    "\n",
    "    # Resize images for non-CIFAR datasets\n",
    "    if dataset in [\"caltech101\", \"stanfordcars\"] and resize is not None:\n",
    "        list_transforms = [transforms.Resize((resize, resize))] + list_transforms\n",
    "    elif dataset == \"svhn\":\n",
    "        list_transforms = [\n",
    "            transforms.Resize((32, 32))\n",
    "        ] + list_transforms  # SVHN images are 32x32\n",
    "    # No resize for Fashion-MNIST (keep 28x28)\n",
    "\n",
    "    transformer = transforms.Compose(list_transforms)\n",
    "\n",
    "    try:\n",
    "        if dataset == \"cifar\":\n",
    "            trainset = datasets.CIFAR10(\n",
    "                data_path + dataset, train=True, download=True, transform=transformer\n",
    "            )\n",
    "            testset = datasets.CIFAR10(\n",
    "                data_path + dataset, train=False, download=True, transform=transformer\n",
    "            )\n",
    "        elif dataset == \"svhn\":\n",
    "            trainset = datasets.SVHN(\n",
    "                data_path + \"svhn\", split=\"train\", download=True, transform=transformer\n",
    "            )\n",
    "            testset = datasets.SVHN(\n",
    "                data_path + \"svhn\", split=\"test\", download=True, transform=transformer\n",
    "            )\n",
    "        elif dataset == \"caltech101\":\n",
    "            trainset = datasets.ImageFolder(\n",
    "                data_path + \"caltech101/train\", transform=transformer\n",
    "            )\n",
    "            testset = datasets.ImageFolder(\n",
    "                data_path + \"caltech101/test\", transform=transformer\n",
    "            )\n",
    "        elif dataset == \"stanfordcars\":\n",
    "            trainset = datasets.StanfordCars(\n",
    "                data_path + \"stanfordcars\",\n",
    "                split=\"train\",\n",
    "                download=True,\n",
    "                transform=transformer,\n",
    "            )\n",
    "            testset = datasets.StanfordCars(\n",
    "                data_path + \"stanfordcars\",\n",
    "                split=\"test\",\n",
    "                download=True,\n",
    "                transform=transformer,\n",
    "            )\n",
    "        elif dataset == \"fashion_mnist\":\n",
    "            trainset = datasets.FashionMNIST(\n",
    "                data_path + \"fashion_mnist\",\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=transformer,\n",
    "            )\n",
    "            testset = datasets.FashionMNIST(\n",
    "                data_path + \"fashion_mnist\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transformer,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dataset: {dataset}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Split data into clients\n",
    "    datasets_train = split_data_client(trainset, num_clients, seed)\n",
    "\n",
    "    # Handle validation data\n",
    "    if data_path_val:\n",
    "        valset = datasets.ImageFolder(data_path_val, transform=transformer)\n",
    "        datasets_val = split_data_client(valset, num_clients, seed)\n",
    "    else:\n",
    "        datasets_val = None\n",
    "\n",
    "    # Create dataloaders\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for i in range(num_clients):\n",
    "        if data_path_val:\n",
    "            trainloaders.append(\n",
    "                DataLoader(datasets_train[i], batch_size=batch_size, shuffle=True)\n",
    "            )\n",
    "            valloaders.append(DataLoader(datasets_val[i], batch_size=batch_size))\n",
    "        else:\n",
    "            len_val = int(len(datasets_train[i]) * splitter / 100)\n",
    "            len_train = len(datasets_train[i]) - len_val\n",
    "            lengths = [len_train, len_val]\n",
    "            ds_train, ds_val = random_split(\n",
    "                datasets_train[i], lengths, torch.Generator().manual_seed(seed)\n",
    "            )\n",
    "            trainloaders.append(\n",
    "                DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "            )\n",
    "            valloaders.append(DataLoader(ds_val, batch_size=batch_size))\n",
    "\n",
    "    testloader = DataLoader(testset, batch_size=batch_size)\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0287a2de",
   "metadata": {
    "papermill": {
     "duration": 0.013334,
     "end_time": "2025-05-18T15:27:09.108428",
     "exception": false,
     "start_time": "2025-05-18T15:27:09.095094",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training and testing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e07334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T15:27:09.136373Z",
     "iopub.status.busy": "2025-05-18T15:27:09.136149Z",
     "iopub.status.idle": "2025-05-18T15:27:09.153390Z",
     "shell.execute_reply": "2025-05-18T15:27:09.152617Z"
    },
    "papermill": {
     "duration": 0.032798,
     "end_time": "2025-05-18T15:27:09.154613",
     "exception": false,
     "start_time": "2025-05-18T15:27:09.121815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: Union[torch.nn.Module, Tuple],\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_proba = []\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images)\n",
    "            probas_output = softmax(output)\n",
    "            y_proba.extend(probas_output.detach().cpu().numpy())\n",
    "            loss = loss_fn(output, labels)\n",
    "            test_loss += loss.item()\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            y_pred.extend(preds)\n",
    "            acc = (preds == labels).mean()\n",
    "            test_acc += acc\n",
    "    y_proba = np.array(y_proba)\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc * 100, y_pred, y_true, y_proba\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: Union[torch.nn.Module, Tuple],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred_class = torch.argmax(torch.softmax(output, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == labels).sum().item() / len(output)\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc * 100\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: torch.utils.data.DataLoader,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Union[torch.nn.Module, Tuple],\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    ") -> Dict[str, List]:\n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(\n",
    "            model, train_dataloader, loss_fn, optimizer, device\n",
    "        )\n",
    "        val_loss, val_acc, *_ = test(model, test_dataloader, loss_fn, device)\n",
    "        print(\n",
    "            f\"\\tTrain Epoch: {epoch + 1} \\tTrain_loss: {train_loss:.4f} | Train_acc: {train_acc:.4f} % | \"\n",
    "            f\"Validation_loss: {val_loss:.4f} | Validation_acc: {val_acc:.4f} %\"\n",
    "        )\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "    return results\n",
    "\n",
    "\n",
    "def serialize_ndarray(ndarray):\n",
    "    if isinstance(ndarray, ts.tensors.CKKSTensor):\n",
    "        return ndarray.serialize()\n",
    "    elif isinstance(ndarray, torch.Tensor):\n",
    "        return serialize_ndarray(ndarray.cpu().detach().numpy())\n",
    "    else:\n",
    "        bytes_io = BytesIO()\n",
    "        np.save(bytes_io, ndarray, allow_pickle=False)\n",
    "        return bytes_io.getvalue()\n",
    "\n",
    "\n",
    "def deserialize_ndarray(tensor, context):\n",
    "    try:\n",
    "        return ts.ckks_tensor_from(context, tensor)\n",
    "    except:\n",
    "        bytes_io = BytesIO(tensor)\n",
    "        return np.load(bytes_io, allow_pickle=False)\n",
    "\n",
    "\n",
    "def serialize_parameters(parameters):\n",
    "    return [serialize_ndarray(param) for param in parameters]\n",
    "\n",
    "\n",
    "def deserialize_parameters(serialized_params, context):\n",
    "    return [deserialize_ndarray(param, context) for param in serialized_params]\n",
    "\n",
    "\n",
    "def privatize_accuracy(true_acc: float, N: int, ε=1.0):\n",
    "    sensitivity = 1.0 / N\n",
    "    noise = np.random.laplace(0, sensitivity / ε)\n",
    "    return np.clip(true_acc + noise, 0, 1)\n",
    "\n",
    "\n",
    "def accuracy_weights(accuracies: List[float], τ=0.5) -> List[float]:\n",
    "    scaled_acc = [a / τ for a in accuracies]\n",
    "    max_scaled = max(scaled_acc)\n",
    "    exp_acc = [np.exp(a - max_scaled) for a in scaled_acc]\n",
    "    sum_exp = sum(exp_acc)\n",
    "    return [e / sum_exp for e in exp_acc]\n",
    "\n",
    "\n",
    "def compute_difference_norm(new_param, prev_param, context):\n",
    "    if isinstance(new_param, ts.tensors.CKKSTensor):\n",
    "        new_dec = new_param.decrypt(context.secret_key()).tolist()\n",
    "        prev_dec = prev_param.decrypt(context.secret_key()).tolist()\n",
    "        diff = np.array(new_dec) - np.array(prev_dec)\n",
    "    else:\n",
    "        if isinstance(new_param, torch.Tensor):\n",
    "            new_param = new_param.cpu().numpy()\n",
    "        if isinstance(prev_param, torch.Tensor):\n",
    "            prev_param = prev_param.cpu().numpy()\n",
    "        diff = new_param - prev_param\n",
    "    return np.linalg.norm(diff)\n",
    "\n",
    "\n",
    "def aggregate_serialized(results, context, τ=0.5):\n",
    "    accuracies = [dp_acc for _, dp_acc in results]\n",
    "    weights = accuracy_weights(accuracies, τ)\n",
    "\n",
    "    weights_results = [\n",
    "        (deserialize_parameters(serialized_params, context), w)\n",
    "        for (serialized_params, _), w in zip(results, weights)\n",
    "    ]\n",
    "\n",
    "    aggregated_params = []\n",
    "    for layer_idx in range(len(weights_results[0][0])):\n",
    "        layer_updates = [weights[layer_idx] for weights, _ in weights_results]\n",
    "        if isinstance(layer_updates[0], ts.tensors.CKKSTensor):\n",
    "            weighted_sum = sum([layer * w for layer, w in zip(layer_updates, weights)])\n",
    "        else:\n",
    "            weighted_sum = sum([layer * w for layer, w in zip(layer_updates, weights)])\n",
    "        aggregated_params.append(weighted_sum)\n",
    "    return serialize_parameters(aggregated_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4745a0",
   "metadata": {
    "papermill": {
     "duration": 0.014153,
     "end_time": "2025-05-18T15:27:09.183077",
     "exception": false,
     "start_time": "2025-05-18T15:27:09.168924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main experiment setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651f9fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T15:27:09.212418Z",
     "iopub.status.busy": "2025-05-18T15:27:09.212198Z",
     "iopub.status.idle": "2025-05-18T15:27:09.289747Z",
     "shell.execute_reply": "2025-05-18T15:27:09.289013Z"
    },
    "papermill": {
     "duration": 0.094091,
     "end_time": "2025-05-18T15:27:09.290944",
     "exception": false,
     "start_time": "2025-05-18T15:27:09.196853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "he = True\n",
    "data_path = \"data/\"\n",
    "dataset = \"fashion_mnist\"\n",
    "yaml_path = \"./results/FL/results.yml\"\n",
    "seed = 42\n",
    "num_workers = 0\n",
    "max_epochs = 10\n",
    "batch_size = 32\n",
    "splitter = 10\n",
    "device = \"gpu\"\n",
    "number_clients = 10\n",
    "save_results = \"results/FL/\"\n",
    "matrix_path = \"confusion_matrix.png\"\n",
    "roc_path = \"roc.png\"\n",
    "model_save = \"fashionmnist_FedAcc.pt\"\n",
    "min_fit_clients = 2\n",
    "min_avail_clients = 2\n",
    "min_eval_clients = 2\n",
    "rounds = 20\n",
    "frac_fit = 1.0\n",
    "frac_eval = 0.5\n",
    "lr = 1e-3\n",
    "path_public_key = \"server_key.pkl\"\n",
    "\n",
    "DEVICE = torch.device(choice_device(device))\n",
    "CLASSES = classes_string(dataset)\n",
    "\n",
    "n_qubits = 4\n",
    "n_layers = 2\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}  # For StronglyEntanglingLayers\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(inputs, weights):\n",
    "    # Automatically pad with zeros and normalize\n",
    "    qml.AmplitudeEmbedding(\n",
    "        features=inputs, wires=range(n_qubits), pad_with=0.0, normalize=True\n",
    "    )\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # Change input channels to 1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # output: 64 x 14 x 14 (28x28 -> 14x14)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # output: 128 x 7 x 7\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # output: 256 x 3 x 3\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 3 * 3, 1024),  # Adjust for 3x3 feature map\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2**n_qubits),\n",
    "        )\n",
    "        self.qnn = qml.qnn.TorchLayer(quantum_net, weight_shapes)\n",
    "        self.fc4 = nn.Linear(n_qubits, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.network(x)\n",
    "        x = self.qnn(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e8ff3",
   "metadata": {
    "papermill": {
     "duration": 0.01413,
     "end_time": "2025-05-18T15:27:09.318855",
     "exception": false,
     "start_time": "2025-05-18T15:27:09.304725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acef28e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T15:27:09.347382Z",
     "iopub.status.busy": "2025-05-18T15:27:09.347124Z",
     "iopub.status.idle": "2025-05-18T19:05:48.220620Z",
     "shell.execute_reply": "2025-05-18T19:05:48.219796Z"
    },
    "papermill": {
     "duration": 13118.889264,
     "end_time": "2025-05-18T19:05:48.222006",
     "exception": false,
     "start_time": "2025-05-18T15:27:09.332742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "secret_path = \"secret.pkl\"\n",
    "public_path = path_public_key\n",
    "if os.path.exists(secret_path):\n",
    "    with open(secret_path, \"rb\") as f:\n",
    "        query = pickle.load(f)\n",
    "    context_client = ts.context_from(query[\"contexte\"])\n",
    "else:\n",
    "    context_client = context()\n",
    "    with open(secret_path, \"wb\") as f:\n",
    "        pickle.dump({\"contexte\": context_client.serialize(save_secret_key=True)}, f)\n",
    "    with open(public_path, \"wb\") as f:\n",
    "        pickle.dump({\"contexte\": context_client.serialize()}, f)\n",
    "context_server = ts.context_from(read_query(public_path)[1])\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(\n",
    "    num_clients=number_clients,\n",
    "    batch_size=batch_size,\n",
    "    resize=None,\n",
    "    seed=seed,\n",
    "    dataset=dataset,\n",
    "    data_path=\"./data/\",\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize global model\n",
    "global_model = Net(num_classes=len(CLASSES)).to(DEVICE)\n",
    "initial_params = get_parameters2(global_model, context_server)\n",
    "global_serialized_params = serialize_parameters(initial_params)\n",
    "\n",
    "\n",
    "# Client training function\n",
    "def client_train(cid, serialized_global_params, local_epochs=max_epochs, lr=lr):\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    local_model = Net(num_classes=len(CLASSES)).to(DEVICE)\n",
    "    params = deserialize_parameters(serialized_global_params, context_client)\n",
    "    set_parameters(local_model, params, context_client)\n",
    "    optimizer = torch.optim.Adam(local_model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    results = train(\n",
    "        local_model,\n",
    "        trainloader,\n",
    "        valloader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        epochs=local_epochs,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    if save_results:\n",
    "        save_graphs(save_results, local_epochs, results, f\"_Client {cid}\")\n",
    "    updated_params = get_parameters2(local_model, context_client)\n",
    "    serialized_updated_params = serialize_parameters(updated_params)\n",
    "    num_examples = len(trainloader.dataset)\n",
    "    val_acc = results[\"val_acc\"][-1] / 100\n",
    "    N_val = len(valloader.dataset)\n",
    "    dp_acc = privatize_accuracy(val_acc, N_val)\n",
    "    print(f\"[Client {cid}] True Acc: {val_acc:.4f}, DP Acc: {dp_acc:.4f}\")\n",
    "    return serialized_updated_params, num_examples, dp_acc\n",
    "\n",
    "\n",
    "# Federated learning simulation\n",
    "print(f\"Training on {DEVICE}\")\n",
    "start_simulation = time.time()\n",
    "\n",
    "layer_names = list(global_model.state_dict().keys())\n",
    "quantum_layer_indices = [i for i, name in enumerate(layer_names) if \"qnn\" in name]\n",
    "previous_aggregated_params = initial_params\n",
    "importance_history = [1.0] * len(initial_params)\n",
    "α = 0.9\n",
    "threshold = 0.001\n",
    "\n",
    "for round_num in range(rounds):\n",
    "    client_updates = []\n",
    "    for cid in range(number_clients):\n",
    "        print(f\"[Client {cid}, round {round_num + 1}] training\")\n",
    "        serialized_updated_params, _, dp_acc = client_train(\n",
    "            str(cid), global_serialized_params\n",
    "        )\n",
    "        client_updates.append((serialized_updated_params, dp_acc))\n",
    "\n",
    "    accuracies = [dp_acc for _, dp_acc in client_updates]\n",
    "    weights = accuracy_weights(accuracies)\n",
    "    print(\n",
    "        f\"[Round {round_num + 1}] Client DP Accuracies: {accuracies}, Weights: {weights}\"\n",
    "    )\n",
    "    global_serialized_params = aggregate_serialized(client_updates, context_server)\n",
    "    new_aggregated_params = deserialize_parameters(\n",
    "        global_serialized_params, context_server\n",
    "    )\n",
    "\n",
    "    differences = [\n",
    "        compute_difference_norm(new, prev, context_client)\n",
    "        for new, prev in zip(new_aggregated_params, previous_aggregated_params)\n",
    "    ]\n",
    "    importance_history = [\n",
    "        α * imp + (1 - α) * diff for imp, diff in zip(importance_history, differences)\n",
    "    ]\n",
    "    print(f\"[Round {round_num + 1}] Importance History: {importance_history}\")\n",
    "\n",
    "    frozen_layers = []\n",
    "    for i, imp in enumerate(importance_history):\n",
    "        if imp < threshold and i not in quantum_layer_indices:\n",
    "            new_aggregated_params[i] = previous_aggregated_params[i]\n",
    "            frozen_layers.append(i)\n",
    "    print(f\"[Round {round_num + 1}] Frozen Layers: {frozen_layers}\")\n",
    "\n",
    "    # Update global model with new aggregated parameters and evaluate\n",
    "    set_parameters(global_model, new_aggregated_params, context_client)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    test_loss, test_acc, _, _, _ = test(global_model, testloader, criterion, DEVICE)\n",
    "    print(\n",
    "        f\"[Round {round_num + 1}] Global Model Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f} %\"\n",
    "    )\n",
    "\n",
    "    global_serialized_params = serialize_parameters(new_aggregated_params)\n",
    "    previous_aggregated_params = new_aggregated_params.copy()\n",
    "    print(f\"Round {round_num + 1} completed\")\n",
    "\n",
    "print(\n",
    "    f\"Federated learning completed. Simulation Time = {time.time() - start_simulation} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d676b",
   "metadata": {
    "papermill": {
     "duration": 0.097139,
     "end_time": "2025-05-18T19:05:48.419891",
     "exception": false,
     "start_time": "2025-05-18T19:05:48.322752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Save the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30acf89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T19:05:48.686593Z",
     "iopub.status.busy": "2025-05-18T19:05:48.686282Z",
     "iopub.status.idle": "2025-05-18T19:05:48.714536Z",
     "shell.execute_reply": "2025-05-18T19:05:48.714004Z"
    },
    "papermill": {
     "duration": 0.128728,
     "end_time": "2025-05-18T19:05:48.715609",
     "exception": false,
     "start_time": "2025-05-18T19:05:48.586881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    os.makedirs(save_results, exist_ok=True)\n",
    "    torch.save({\"model_state_dict\": global_model.state_dict()}, model_save)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13224.972648,
   "end_time": "2025-05-18T19:05:51.643938",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-18T15:25:26.671290",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
