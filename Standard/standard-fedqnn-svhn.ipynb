{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3129c039",
   "metadata": {
    "papermill": {
     "duration": 0.004145,
     "end_time": "2025-05-11T16:18:40.024857",
     "exception": false,
     "start_time": "2025-05-11T16:18:40.020712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install and Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf9f1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T16:32:13.880812Z",
     "iopub.status.busy": "2025-05-24T16:32:13.880524Z",
     "iopub.status.idle": "2025-05-24T16:32:16.945902Z",
     "shell.execute_reply": "2025-05-24T16:32:16.944916Z",
     "shell.execute_reply.started": "2025-05-24T16:32:13.880791Z"
    },
    "papermill": {
     "duration": 68.431228,
     "end_time": "2025-05-11T16:19:48.459573",
     "exception": false,
     "start_time": "2025-05-11T16:18:40.028345",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202aa2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T16:32:16.948490Z",
     "iopub.status.busy": "2025-05-24T16:32:16.948230Z",
     "iopub.status.idle": "2025-05-24T16:32:16.955410Z",
     "shell.execute_reply": "2025-05-24T16:32:16.954670Z",
     "shell.execute_reply.started": "2025-05-24T16:32:16.948467Z"
    },
    "papermill": {
     "duration": 32.824427,
     "end_time": "2025-05-11T16:20:21.297442",
     "exception": false,
     "start_time": "2025-05-11T16:19:48.473015",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pennylane as qml\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Union, Tuple\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56315d40",
   "metadata": {
    "papermill": {
     "duration": 0.013525,
     "end_time": "2025-05-11T16:20:21.325113",
     "exception": false,
     "start_time": "2025-05-11T16:20:21.311588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc557baf-0e2e-4272-8df7-cd53e0df98fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T16:32:16.956530Z",
     "iopub.status.busy": "2025-05-24T16:32:16.956280Z",
     "iopub.status.idle": "2025-05-24T16:32:16.971465Z",
     "shell.execute_reply": "2025-05-24T16:32:16.970704Z",
     "shell.execute_reply.started": "2025-05-24T16:32:16.956514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def aggregate_weights(client_weights, client_sizes):\n",
    "    global_weights = {}\n",
    "    total_size = sum(client_sizes)\n",
    "\n",
    "    # Initialize with zeros\n",
    "    for key in client_weights[0].keys():\n",
    "        global_weights[key] = torch.zeros_like(client_weights[0][key])\n",
    "\n",
    "    # Accumulate weighted updates\n",
    "    for idx, (weights, size) in enumerate(zip(client_weights, client_sizes)):\n",
    "        weight_factor = size / total_size\n",
    "        for key in weights.keys():\n",
    "            global_weights[key] += weights[key] * weight_factor\n",
    "\n",
    "    return global_weights\n",
    "\n",
    "\n",
    "def federated_learning():\n",
    "    # Load datasets\n",
    "    trainloaders, _, testloader = load_datasets(\n",
    "        num_clients=CONFIG[\"num_clients\"],\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        resize=CONFIG[\"resize\"],\n",
    "        seed=CONFIG[\"seed\"],\n",
    "        num_workers=CONFIG[\"num_workers\"],\n",
    "        splitter=CONFIG[\"splitter\"],\n",
    "        dataset=CONFIG[\"dataset\"],\n",
    "        data_path=CONFIG[\"data_path\"],\n",
    "        data_path_val=CONFIG[\"data_path_val\"],\n",
    "    )\n",
    "\n",
    "    # Get client dataset sizes\n",
    "    client_sizes = [len(loader.dataset) for loader in trainloaders]\n",
    "\n",
    "    # Initialize global model and loss function\n",
    "    global_model = QNNModel().to(CONFIG[\"device\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initial evaluation\n",
    "    init_loss, init_acc, _, _, _ = test(\n",
    "        global_model, testloader, criterion, CONFIG[\"device\"]\n",
    "    )\n",
    "    print(f\"\\nInitial Global Model - Loss: {init_loss:.4f}, Accuracy: {init_acc:.2f}%\")\n",
    "\n",
    "    # Federated learning loop\n",
    "    for round in range(CONFIG[\"num_rounds\"]):\n",
    "        print(f\"\\n=== Federated Round {round+1}/{CONFIG['num_rounds']} ===\")\n",
    "\n",
    "        # Client training\n",
    "        client_weights = []\n",
    "        for client_id in range(CONFIG[\"num_clients\"]):\n",
    "            print(f\"\\n--- Client {client_id+1} Training ---\")\n",
    "            model = QNNModel().to(CONFIG[\"device\"])\n",
    "            model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "            weights = train(\n",
    "                model,\n",
    "                trainloaders[client_id],\n",
    "                epochs=CONFIG[\"local_epochs\"],\n",
    "                lr=CONFIG[\"learning_rate\"],\n",
    "                device=CONFIG[\"device\"],\n",
    "            )\n",
    "            client_weights.append(weights)\n",
    "\n",
    "        # Aggregate weights\n",
    "        global_weights = aggregate_weights(client_weights, client_sizes)\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        # Global evaluation\n",
    "        test_loss, test_acc, _, _, _ = test(\n",
    "            global_model, testloader, criterion, CONFIG[\"device\"]\n",
    "        )\n",
    "        print(\n",
    "            f\"\\nGlobal Model Performance, Round: {round+1} - Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383e573",
   "metadata": {
    "papermill": {
     "duration": 0.013965,
     "end_time": "2025-05-11T16:20:21.466953",
     "exception": false,
     "start_time": "2025-05-11T16:20:21.452988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0ea11-c45b-471d-98be-555663607b2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T16:32:16.972896Z",
     "iopub.status.busy": "2025-05-24T16:32:16.972697Z",
     "iopub.status.idle": "2025-05-24T16:32:16.991797Z",
     "shell.execute_reply": "2025-05-24T16:32:16.991072Z",
     "shell.execute_reply.started": "2025-05-24T16:32:16.972873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NORMALIZE_DICT = {\n",
    "    \"svhn\": dict(mean=(0.4377, 0.4438, 0.4728), std=(0.1980, 0.2010, 0.1970)),\n",
    "}\n",
    "\n",
    "\n",
    "def split_data_client(dataset, num_clients, seed):\n",
    "    partition_size = len(dataset) // num_clients\n",
    "    lengths = [partition_size] * (num_clients - 1)\n",
    "    lengths += [len(dataset) - sum(lengths)]\n",
    "    ds = random_split(dataset, lengths, torch.Generator().manual_seed(seed))\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    num_clients: int,\n",
    "    batch_size: int,\n",
    "    resize: int,\n",
    "    seed: int,\n",
    "    num_workers: int,\n",
    "    splitter=10,\n",
    "    dataset=\"svhn\",\n",
    "    data_path=\"./data/\",\n",
    "    data_path_val=\"\",\n",
    "):\n",
    "    list_transforms = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**NORMALIZE_DICT[dataset]),\n",
    "    ]\n",
    "\n",
    "    # Resize images for non-CIFAR datasets\n",
    "    if dataset in [\"caltech101\", \"stanfordcars\"] and resize is not None:\n",
    "        list_transforms = [transforms.Resize((resize, resize))] + list_transforms\n",
    "    elif dataset == \"svhn\":\n",
    "        list_transforms = [\n",
    "            transforms.Resize((32, 32))\n",
    "        ] + list_transforms  # SVHN images are 32x32\n",
    "\n",
    "    transformer = transforms.Compose(list_transforms)\n",
    "\n",
    "    if dataset == \"cifar\":\n",
    "        trainset = datasets.CIFAR10(\n",
    "            data_path + dataset, train=True, download=True, transform=transformer\n",
    "        )\n",
    "        testset = datasets.CIFAR10(\n",
    "            data_path + dataset, train=False, download=True, transform=transformer\n",
    "        )\n",
    "    elif dataset == \"svhn\":\n",
    "        trainset = datasets.SVHN(\n",
    "            data_path + \"svhn\", split=\"train\", download=True, transform=transformer\n",
    "        )\n",
    "        testset = datasets.SVHN(\n",
    "            data_path + \"svhn\", split=\"test\", download=True, transform=transformer\n",
    "        )\n",
    "    else:\n",
    "        trainset = datasets.ImageFolder(\n",
    "            data_path + dataset + \"/Training\", transform=transformer\n",
    "        )\n",
    "        testset = datasets.ImageFolder(\n",
    "            data_path + dataset + \"/Testing\", transform=transformer\n",
    "        )\n",
    "\n",
    "    datasets_train = split_data_client(trainset, num_clients, seed)\n",
    "\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for i in range(num_clients):\n",
    "        if data_path_val:\n",
    "            valset = datasets.ImageFolder(data_path_val, transform=transformer)\n",
    "            datasets_val = split_data_client(valset, num_clients, seed)\n",
    "            trainloaders.append(\n",
    "                DataLoader(datasets_train[i], batch_size=batch_size, shuffle=True)\n",
    "            )\n",
    "            valloaders.append(DataLoader(datasets_val[i], batch_size=batch_size))\n",
    "        else:\n",
    "            len_val = int(len(datasets_train[i]) * splitter / 100)\n",
    "            len_train = len(datasets_train[i]) - len_val\n",
    "            lengths = [len_train, len_val]\n",
    "            ds_train, ds_val = random_split(\n",
    "                datasets_train[i], lengths, torch.Generator().manual_seed(seed)\n",
    "            )\n",
    "            trainloaders.append(\n",
    "                DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "            )\n",
    "            valloaders.append(DataLoader(ds_val, batch_size=batch_size))\n",
    "\n",
    "    testloader = DataLoader(testset, batch_size=CONFIG[\"test_batch_size\"])\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cfea6",
   "metadata": {
    "papermill": {
     "duration": 0.016003,
     "end_time": "2025-05-11T16:20:21.534468",
     "exception": false,
     "start_time": "2025-05-11T16:20:21.518465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training and testing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71832305-5abb-40a0-bee4-f34df15f13ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T16:32:16.992914Z",
     "iopub.status.busy": "2025-05-24T16:32:16.992707Z",
     "iopub.status.idle": "2025-05-24T16:32:17.014955Z",
     "shell.execute_reply": "2025-05-24T16:32:17.014310Z",
     "shell.execute_reply.started": "2025-05-24T16:32:16.992899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, trainloader, epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss/len(trainloader):.4f}, Acc: {epoch_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "    return model.state_dict()\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: Union[torch.nn.Module, Tuple],\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_proba = []\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images)\n",
    "\n",
    "            probas_output = softmax(output)\n",
    "            y_proba.extend(probas_output.detach().cpu().numpy())\n",
    "\n",
    "            loss = loss_fn(output, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            labels_np = labels.data.cpu().numpy()\n",
    "            y_true.extend(labels_np)\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "            acc = (preds == labels_np).mean()\n",
    "            test_acc += acc\n",
    "\n",
    "    y_proba = np.array(y_proba)\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc * 100, y_pred, y_true, y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b364de",
   "metadata": {
    "papermill": {
     "duration": 0.015494,
     "end_time": "2025-05-11T16:20:21.618058",
     "exception": false,
     "start_time": "2025-05-11T16:20:21.602564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main experiment setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce27f61-2cd4-4464-b99a-98e3844a986e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T16:32:17.015995Z",
     "iopub.status.busy": "2025-05-24T16:32:17.015746Z",
     "iopub.status.idle": "2025-05-24T16:32:17.095774Z",
     "shell.execute_reply": "2025-05-24T16:32:17.095085Z",
     "shell.execute_reply.started": "2025-05-24T16:32:17.015973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"dataset\": \"svhn\",\n",
    "    \"num_clients\": 10,\n",
    "    \"num_rounds\": 20,\n",
    "    \"local_epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"test_batch_size\": 256,\n",
    "    \"resize\": 32,\n",
    "    \"seed\": 42,\n",
    "    \"num_workers\": 0,\n",
    "    \"splitter\": 10,\n",
    "    \"data_path\": \"./data/\",\n",
    "    \"data_path_val\": \"\",\n",
    "    \"num_classes\": 10,\n",
    "    \"n_qubits\": 6,\n",
    "    \"n_layers\": 6,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "}\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=CONFIG[\"n_qubits\"])\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(inputs, weights):\n",
    "    qml.AngleEmbedding(inputs, wires=range(CONFIG[\"n_qubits\"]))\n",
    "    qml.BasicEntanglerLayers(weights, wires=range(CONFIG[\"n_qubits\"]))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(CONFIG[\"n_qubits\"])]\n",
    "\n",
    "\n",
    "class QNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, CONFIG[\"n_qubits\"]),\n",
    "        )\n",
    "\n",
    "        self.qnn = qml.qnn.TorchLayer(\n",
    "            quantum_net, {\"weights\": (CONFIG[\"n_layers\"], CONFIG[\"n_qubits\"])}\n",
    "        )\n",
    "        self.fc = nn.Linear(CONFIG[\"n_qubits\"], CONFIG[\"num_classes\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.qnn(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb649a",
   "metadata": {
    "papermill": {
     "duration": 0.01309,
     "end_time": "2025-05-11T16:20:21.747680",
     "exception": false,
     "start_time": "2025-05-11T16:20:21.734590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b49e5-f9b6-4387-a364-57bb252bb143",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T16:32:17.096692Z",
     "iopub.status.busy": "2025-05-24T16:32:17.096448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "print(\"Starting federated learning...\")\n",
    "start_time = time.time()\n",
    "federated_learning()\n",
    "print(f\"Total training time: {time.time()-start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12144.906721,
   "end_time": "2025-05-11T19:41:00.454287",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-11T16:18:35.547566",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
